---
title: "Python Code Metrics"
sidebarTitle: "Python Code"
icon: "code"
iconType: "solid"
---

## Python Code Metrics

Python Code Metrics allow you to write custom evaluation logic in Python to evaluate your AI agent's performance. This gives you complete control over the evaluation process and enables complex analysis that goes beyond simple prompt-based metrics.

### Overview

Custom code metrics are executed in a secure Python environment with access to call data including transcripts, metadata, and dynamic variables. Your code must set specific output variables to provide the evaluation result and explanation.

### Available Data Variables

When writing your custom code, you have access to the following data variables:

<AccordionGroup>
  <Accordion title="transcript" href="#transcript">
    <code>data["transcript"]</code> - Full conversation transcript as a string
  </Accordion>
  <Accordion title="transcript_json" href="#transcript-json">
    <code>data["transcript_json"]</code> - Transcript as a structured list with
    speaker information
  </Accordion>
  <Accordion title="metadata" href="#metadata">
    <code>data["metadata"]</code> - Additional metadata as a dictionary
  </Accordion>
  <Accordion title="dynamic_variables" href="#dynamic-variables">
    <code>data["dynamic_variables"]</code> - Dynamic variables as a dictionary
  </Accordion>
  <Accordion title="call_duration" href="#call-duration">
    <code>data["call_duration"]</code> - Call duration in seconds as an integer
  </Accordion>
  <Accordion title="tags" href="#tags">
    <code>data["tags"]</code> - Tags associated with the call as a list
  </Accordion>
  <Accordion title="call_end_reason" href="#call-end-reason">
    <code>data["call_end_reason"]</code> - Reason why the call ended (e.g.,
    "hangup", "timeout", "error")
  </Accordion>
  <Accordion title="metric_results" href="#metric-results">
    <code>data["metric_results"]</code> - Results from other Basic, Advanced,
    and pre-defined metrics evaluated for this call. You can also access
    individual metric results directly using{" "}
    <code>data[&lt;metric_name&gt;]</code>
  </Accordion>
</AccordionGroup>

### Required Output Variables

Your Python code must set these two variables:

- **`_result`** - The evaluation outcome (can be boolean, numeric, string, etc.)
- **`_explanation`** - A string explaining the reasoning behind the result

### Example Code

Here's a simple example that checks if the agent mentioned a specific product:

```python
# Check if the agent mentioned "Premium Plan" in the conversation
transcript = data["transcript"].lower()
if "premium plan" in transcript:
    _result = True
    _explanation = "Agent successfully mentioned the Premium Plan during the conversation"
else:
    _result = False
    _explanation = "Agent did not mention the Premium Plan in the conversation"
```

### Using Metric Results

You can access the results of other metrics that were evaluated for the same call directly by metric name using `data[<metric_name>]`. This allows you to create complex evaluations that depend on the outcomes of other metrics.

Example usage:

```python
# Access metric results directly by name
customer_satisfaction = data["Customer Satisfaction"]
response_time = data["Response Time"]
product_knowledge = data["Product Knowledge"]

# Each metric result contains the evaluation outcome
if customer_satisfaction and response_time < 60:
    _result = "Excellent"
    _explanation = "Customer was satisfied and response time was fast"
```


### Example for Calling New Binary And Advanced Metrics

To call new Binary and Advanced metrics from your Python code, you can use the `evaluate_advance_metric` function. This function requires your API key, a description or prompt for the metric, and the metric name.

#### Example For Calling Basic Metrics

```python
key = "<your_cekura_api_key"

def get_not_early_end_call_description():
    return f"""You are an AI quality assurance analyst tasked with evaluating customer service call transcripts. Your primary objective is to determine if a Main Agent terminated a call prematurely without valid reason. This analysis is crucial for maintaining high standards in customer service interactions."""

call_end_reason = data["call_end_reason"]
transcript_json = data["transcript_json"]

if "main" not in call_end_reason.lower():
    _score = 5
    _explanation = "The call was ended by the Testing Agent or due to error."

description = get_not_early_end_call_description()

response = evaluate_advance_metric(data, key, description, "binary_workflow_adherence")
_result = response.get("result")
_explanation = response.get("explanation")
```

#### Example For Calling Advanced Metrics

```python

key = "<your_cekura_api_key"

def get_not_early_end_call_prompt(transcript, call_end_reason):
    return f"""You are an AI quality assurance analyst tasked with evaluating customer service call transcripts. Your primary objective is to determine if a Main Agent terminated a call prematurely without valid reason. This analysis is crucial for maintaining high standards in customer service interactions.

Please review the following call transcript:

<call_transcript>
{transcript}
</call_transcript>

Now, consider the reason provided for why the call ended:

<call_end_reason>
{call_end_reason}
</call_end_reason>

Your task is to analyze the transcript and call end reason to determine if the Main Agent terminated the call early without justification. 
"""

if "transcript_json" not in data or not data["transcript_json"]:
    _score = None
    _explanation = "No transcript available"

if "call_end_reason" not in data or not data["call_end_reason"]:
    _score = None
    _explanation = "No call end reason available"


call_end_reason = data["call_end_reason"]
transcript_json = data["transcript_json"]

if "main" not in call_end_reason.lower():
    _score = 5
    _explanation = "The call was ended by the Testing Agent or due to error."

prompt = get_not_early_end_call_prompt(data["transcript"], data["call_end_reason"])

response = evaluate_advance_metric(data, key, prompt, "binary_workflow_adherence")
_result = response.get("result")
_explanation = response.get("explanation")
```

### Advanced Example

Here's a more complex example that analyzes sentiment and response time:

```python
import re
from datetime import datetime

# Get transcript data
transcript = data["transcript"]
call_duration = data["call_duration"]

# Analyze agent responses
agent_responses = []
lines = transcript.split('\n')

for line in lines:
    if line.strip().startswith('Agent:'):
        response = line.replace('Agent:', '').strip()
        agent_responses.append(response)

# Calculate average response length
if agent_responses:
    avg_response_length = sum(len(response) for response in agent_responses) / len(agent_responses)

    # Check if responses are detailed enough (more than 50 characters average)
    if avg_response_length > 50:
        _result = True
        _explanation = f"Agent provided detailed responses with average length of {avg_response_length:.1f} characters"
    else:
        _result = False
        _explanation = f"Agent responses were too brief with average length of {avg_response_length:.1f} characters"
else:
    _result = False
    _explanation = "No agent responses found in transcript"
```

### Example Using Multiple Data Sources

Here's an example that combines multiple metric results with call metadata and tags:

```python
# Access metric results directly by name
try:
    satisfaction = data["Customer Satisfaction"]
    response_time = data["Response Time"]

    # Access additional call data
    call_duration = data["call_duration"]
    call_ended_reason = data["call_ended_reason"]
    tags = data["tags"]

    # Check if this was a priority call based on tags
    is_priority = "priority" in tags or "vip" in tags

    # Evaluate based on multiple factors
    if call_ended_reason == "hangup" and satisfaction and response_time < 60:
        if is_priority:
            _result = "Excellent"
            _explanation = f"Priority customer was satisfied ({satisfaction}) with fast response time ({response_time}s) and completed the call normally"
        else:
            _result = "Good"
            _explanation = f"Customer was satisfied ({satisfaction}) with fast response time ({response_time}s) and completed the call normally"
    elif call_ended_reason in ["timeout", "error"]:
        _result = "Poor"
        _explanation = f"Call ended unexpectedly due to {call_ended_reason}, indicating technical issues"
    else:
        _result = "Needs Improvement"
        _explanation = f"Call performance needs improvement - satisfaction: {satisfaction}, response time: {response_time}s, ended reason: {call_ended_reason}"

except KeyError as e:
    _result = "Incomplete"
    _explanation = f"Required data not found: {str(e)}"
```

### Accessing Latency Data

Access latency data to analyze voice agent response timing performance. This data is available only when the call is evaluated with a Latency metric.

#### Available Latency Variables

<AccordionGroup>
  <Accordion title="Latency (in ms)" href="#latency">
    <code>data["Latency (in ms)"]</code> - Overall average latency for the entire call in milliseconds
  </Accordion>
  <Accordion title="latency_data" href="#latency-data">
    <code>data["latency_data"]</code> - Array of individual response latency measurements with detailed timing information
  </Accordion>
</AccordionGroup>

#### Example Latency Data Structure

```json
{
  "Latency (in ms)": 2240.0,
  "latency_data": [
    {
      "latency": 1570.0,
      "speaker": "Main Agent",
      "start_time": 4.61
    },
    {
      "latency": 2910.0,
      "speaker": "Main Agent",
      "start_time": 16.09
    }
  ]
}
```

**Field Descriptions:**
- `Latency (in ms)`: Overall average latency (2240.0 milliseconds in this example)
- `latency_data`: Array containing individual response
- `latency`: Response time in milliseconds for each interaction
- `start_time`: When the response started in seconds from call beginning

#### Example Code Using Latency Data

```python
# Access overall latency
overall_latency = data.get("Latency (in ms)", 0)

# Access detailed latency measurements
latency_data = data.get("latency_data", [])

# Check if latency data is available
if not latency_data:
    _result = False
    _explanation = "No latency data available"
else:
    # Process individual measurements
    high_latency_responses = []
    for measurement in latency_data:
        latency_ms = measurement["latency"]
        speaker = measurement["speaker"]
        start_time = measurement["start_time"]
        
        # Check if latency exceeds threshold (3 seconds)
        if latency_ms > 3000:
            high_latency_responses.append(f"{speaker}: {latency_ms}ms at {start_time}s")
    
    if high_latency_responses:
        _result = False
        _explanation = f"High latency detected in {len(high_latency_responses)} responses: {', '.join(high_latency_responses)}"
    else:
        _result = True
        _explanation = f"All responses within acceptable range. Average: {overall_latency}ms"
```
