---
title: "Metric Optimizer"
sidebarTitle: "Metric Optimizer"
icon: "gauge-high"
iconType: "regular"
description: "Metric Optimizer helps you define, test, and refine custom metrics for your AI agents to ensure accurate performance measurement."
---

<iframe
  width="100%"
  height="400px"
  style={{
    aspectRatio: "16 / 9",
    border: "1px solid #ccc",
    borderRadius: "8px",
    marginBottom: "20px",
  }}
  src="https://www.tella.tv/video/cm7vbj9p0001k0ajo6ex71u1o/embed"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
  title="Metric Optimizer Overview"
/>

## Advanced Features of Metric Optimizer

Metric Optimizer lets you define, test, and optimize custom metrics for your AI assistant. This guide covers refining metrics to accurately reflect real-world performance.

### Defining Custom Metrics

Create custom metrics tailored to your use cases. For example, a "Time Zone Confirmation" metric could check if your AI assistant confirms the time zone before scheduling a call.

<img src="/images/metriclab/timezone.png" alt="Time Zone Confirmation" className="rounded-md" />

### Identifying Metric Performance Issues

After deploying metrics, you may notice discrepancies between expected and actual results:

- The AI doesn't confirm the time zone, yet the metric marks it as successful
- The AI confirms the time zone, but the metric marks it as unsuccessful

These inconsistencies indicate your metric definition needs refinement.

### Creating Test Sets for Optimization

When you find calls where metrics aren't performing as expected, add them to a test set directly from the interface.

#### Step 1: Add problematic calls to a test set

Select calls that aren't being evaluated correctly and add them to a test set.

<img src="/images/metriclab/addtolab.png" alt="Add to Test Set" className="rounded-md" />

#### Step 2: Set expected values

In the optimizer view, set the expected outcome for each call:

1. Review each call recording
2. Determine what actually happened (e.g., "AI did not confirm the time zone")
3. Set the expected value (e.g., set to "false" if the AI didn't confirm)
4. Your changes are saved automatically

Click the **Variables** button to view all variables and their values used during metric evaluation.

### Optimizing Your Metrics

Once you've built a test set with correct expected values, refine your metric definition:

#### Step 1: Navigate to the optimization section

Go to **Optimize** and select the metric you want to improve.

<img src="/images/metriclab/optimizer.png" alt="Optimize Metric" className="rounded-md" />

#### Step 2: Edit the metric prompt

The interface shows your current accuracy score (e.g., "6 out of 8"), indicating how many test cases your metric correctly evaluates.

#### Step 3: Refine the metric definition

Edit the metric prompt to be more precise. For example, specify that the user must mention a specific time zone for the metric to pass.

<img src="/images/metriclab/promt.png" alt="Accuracy" className="rounded-md" />

#### Step 4: Test and iterate

After editing:

1. Save your changes
2. Run the updated metric against your test set
3. Observe the improvement in accuracy (e.g., from 6/8 to 7/8 or 8/8)
4. Continue refining until you reach your desired accuracy

### Managing Test Sets

Manage test sets directly from the optimizer view:

- **Delete test sets**: Select test sets using the checkboxes, then click **Delete Selected**
- **Remove variables**: Hover over any variable badge and click the X button to remove it

### Benefits of Metric Optimization

With this iterative process you can:

- Improve metric accuracy from 50% to 95% or higher
- Ensure your AI assistant's labels accurately reflect real performance
- Make data-driven decisions based on reliable metrics
