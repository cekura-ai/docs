---
title: 'Listing Metrics'
description: 'Learn how to retrieve a list of metrics for your agents'
---

# List Metrics API
Retrieve a list of metrics for an agent

## API Endpoint

| Method | Endpoint |
|--------|----------|
| `GET` | `https://new-prod.cekura.ai/test_framework/v1/metrics-external/` |

## Authentication

Include your API key in the request headers:

| Header | Description |
|--------|-------------|
| `X-VOCERA-API-KEY` | Your API key obtained from the dashboard |

## Query Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `agent_id` | integer | Yes* | Filter metrics by agent ID |
| `assistant_id` | string | Yes* | Filter metrics by assistant ID associated with agent |

\* Only either of `agent_id` or `assistant_id` is required

## Response Format

The API returns a list of metric objects.

### Metric Object

| Field | Type | Description |
|-------|------|-------------|
| `id` | integer | Unique identifier for the metric |
| `agent` | integer | ID of the associated agent |
| `name` | string | Name of the metric |
| `description` | string | Description of what the metric evaluates |
| `function_name` | string\|null | Name of evaluation function |
| `eval_type` | string | Type of metric (binary_workflow_adherence, binary_qualitative, continuous_qualitative, numeric, enum) |
| `enum_values` | array | List of possible values when eval_type is "enum" |
| `audio_enabled` | boolean | Whether audio analysis is enabled |
| `prompt_enabled` | boolean | Whether custom prompt is enabled |
| `prompt` | string | Custom evaluation prompt template when prompt_enabled is true |
| `display_order` | integer | Order for displaying the metric |
| `overall_score` | number | Total testsets passed (null if never run) |
| `total_score` | number | Reviewed in total testsets |

### Example Response

```json
[
    {
        "id": 1,
        "agent": 1,
        "name": "Annoyance level",
        "description": "How annoyed were they?",
        "function_name": null,
        "eval_type": "enum",
        "enum_values": [
            "Very",
            "Little", 
            "Not at-all"
        ],
        "audio_enabled": false,
        "prompt_enabled": false,
        "prompt": "",
        "display_order": 1,
        "overall_score": 0,
        "total_score": 11
    }
    // Additional metrics...
]
```

## Code Examples

<CodeGroup>

```bash Curl
curl -X GET "https://new-prod.cekura.ai/test_framework/v1/metrics-external/?agent_id=1" \
  -H "X-VOCERA-API-KEY: <your-api-key-here>"
```

```python Python
import requests

def list_metrics(api_key, agent_id):
    url = 'https://new-prod.cekura.ai/test_framework/v1/metrics-external/'
    headers = {
        'X-VOCERA-API-KEY': api_key
    }
    params = {
        'agent_id': agent_id
    }

    response = requests.get(url, headers=headers, params=params)
    return response.json()
```

```javascript JavaScript
async function listMetrics(apiKey, agentId) {
  const url = new URL('https://new-prod.cekura.ai/test_framework/v1/metrics-external/');
  url.searchParams.append('agent_id', agentId);

  const response = await fetch(url, {
    method: 'GET',
    headers: {
      'X-VOCERA-API-KEY': apiKey
    },
  });

  return await response.json();
}
```

</CodeGroup>
