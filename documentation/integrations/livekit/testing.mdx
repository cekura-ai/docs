---
title: 'LiveKit (Automated)'
sidebarTitle: "LiveKit Automated"
icon: "robot"
iconType: "regular"
description: 'Test agents with automated LiveKit room creation and token generation'
---

## Overview

Test your LiveKit agents with automated room and token management. Cekura handles room creation and token generation automatically.

<Tabs>
<Tab title="No-code">

Run tests directly from the frontend without writing code.

<Steps>
<Step title="Configure LiveKit credentials">
Go to your agent settings and configure LiveKit integration:

<img src="/images/livekit/livekit-voice-integration-config.png" alt="LiveKit Configuration" />

**Required fields:**
- **Provider**: Select LiveKit from the dropdown
- **LiveKit API Key**: Your LiveKit API key
- **LiveKit API Secret**: Your LiveKit API secret
- **LiveKit URL**: Your LiveKit server URL (e.g., `wss://your-server.livekit.cloud`)

**Optional fields:**
- **Agent Name (Optional)**: The specific agent name to dispatch in LiveKit

<Note>
**When to use Agent Name:**
Use this field when you have configured a specific agent name in your LiveKit agent setup. When provided, Cekura will dispatch to agents matching this exact name during test execution.

Leave this field empty if:
- You're using the default agent dispatching behavior
- Your LiveKit setup doesn't use named agents
- You want any available agent to handle the request
</Note>

- **LiveKit Config (JSON)**: Additional room configuration parameters

<Note>
**Accessing Config in Your Agent:**
Config parameters you might be using in your livekit agent's code. The configuration JSON is stored in the LiveKit room's metadata. Access it in your agent's entrypoint:
```python
import json
from livekit.agents import JobContext

async def entrypoint(ctx: JobContext):
    await ctx.connect()

    # Access room metadata
    room_metadata = ctx.room.metadata
    config = json.loads(room_metadata) if room_metadata else {}

    # Use config values
    empty_timeout = config.get("empty_timeout", 300)
    max_participants = config.get("max_participants", 10)
```
</Note>

</Step>

<Step title="Run tests from frontend">
Select scenarios and run tests using the UI:

<img src="/images/livekit/run_evaluaters_via_livekit.png" alt="Run LiveKit Tests" />

Click **"Run with LiveKit"** to execute your tests. Cekura automatically:
- Creates unique rooms for each scenario
- Generates access tokens
- Executes tests and cleans up resources
</Step>

<Step title="View results">
Results appear in your dashboard. Track test status, metrics, and conversation details in real-time.
</Step>
</Steps>

</Tab>

<Tab title="Code">

Use the API to integrate LiveKit testing into your workflow.

## Prerequisites

Configure LiveKit credentials in your agent settings:

<img src="/images/livekit/livekit-voice-integration-config.png" alt="LiveKit Configuration" />

**Required fields:**
- **Provider**: Select LiveKit from the dropdown
- **LiveKit API Key**: Your LiveKit API key
- **LiveKit API Secret**: Your LiveKit API secret
- **LiveKit URL**: Your LiveKit server URL (e.g., `wss://your-server.livekit.cloud`)

**Optional fields:**
- **Agent Name (Optional)**: The specific agent name to dispatch in LiveKit

<Note>
**When to use Agent Name:**
Use this field when you have configured a specific agent name in your LiveKit agent setup. When provided, Cekura will dispatch to agents matching this exact name during test execution.

Leave this field empty if:
- You're using the default agent dispatching behavior
- Your LiveKit setup doesn't use named agents
- You want any available agent to handle the request
</Note>

- **LiveKit Config (JSON)**: Additional room configuration parameters

<Note>
**Accessing Config in Your Agent:**
Config parameters you might be using in your livekit agent's code. The configuration JSON is stored in the LiveKit room's metadata. Access it in your agent's entrypoint:
```python
import json
from livekit.agents import JobContext

async def entrypoint(ctx: JobContext):
    await ctx.connect()

    # Access room metadata
    room_metadata = ctx.room.metadata
    config = json.loads(room_metadata) if room_metadata else {}

    # Use config values
    empty_timeout = config.get("empty_timeout", 300)
    max_participants = config.get("max_participants", 10)
```
</Note>

## API Endpoint

```
POST https://api.cekura.ai/test_framework/v1/scenarios-external/run_scenarios_livekit_v2/
```

## Authentication

Include your API key in the request header:
```
X-CEKURA-API-KEY: <YOUR_API_KEY>
```

## Request Parameters

**scenarios** (array, required): Array of test configurations

Each scenario object contains:
- **scenario** (number, required): Scenario ID

**frequency** (number, optional): Number of times to run each scenario. Defaults to 1 if not specified.

## Examples

### Single Run (cURL)

```bash
curl -X POST \
  'https://api.cekura.ai/test_framework/v1/scenarios-external/run_scenarios_livekit_v2/' \
  -H 'X-CEKURA-API-KEY: <YOUR_API_KEY>' \
  -H 'Content-Type: application/json' \
  -d '{
    "scenarios": [
      {
        "scenario": 30
      }
    ],
    "frequency": 1
  }'
```

### Multiple Runs (cURL)

```bash
curl -X POST \
  'https://api.cekura.ai/test_framework/v1/scenarios-external/run_scenarios_livekit_v2/' \
  -H 'X-CEKURA-API-KEY: <YOUR_API_KEY>' \
  -H 'Content-Type: application/json' \
  -d '{
    "scenarios": [
      {
        "scenario": 30
      },
      {
        "scenario": 31
      }
    ],
    "frequency": 1
  }'
```

### Python Example

```python
import requests

API_KEY = "<YOUR_API_KEY>"
BASE_URL = "https://api.cekura.ai/test_framework"

headers = {
    "X-CEKURA-API-KEY": API_KEY,
    "Content-Type": "application/json",
}

payload = {
    "scenarios": [
        {
            "scenario": 30,
        },
        {
            "scenario": 31,
        },
    ],
    "frequency": 1,
}

resp = requests.post(
    f"{BASE_URL}/v1/scenarios-external/run_scenarios_livekit_v2/",
    headers=headers,
    json=payload,
)
result = resp.json()
print(result)
```

## Response

```json
{
  "id": 16870,
  "name": "",
  "agent": 5,
  "status": "in_progress",
  "success_rate": 0.0,
  "run_as_text": false,
  "is_cronjob": false,
  "runs": [
    {
      "id": 34625,
      "status": "running",
      "scenario": 11547,
      "scenario_name": "In-Person Chronic Care Refusal",
      "test_profile_data": {"key": "value"}
    }
  ],
  "created_at": "2025-10-16T09:32:59.484534Z",
  "updated_at": "2025-10-16T09:32:59.484942Z"
}
```

## Monitor Results

Poll for results using the [List Runs with IDs API](/api-reference/test_framework/list-runs-with-ids).

</Tab>
</Tabs>

## Next Steps

- [Custom metrics](/documentation/key-concepts/metrics/custom-metrics)
- [Instruction following metric](/documentation/key-concepts/metrics/instruction-following-metric)
- [Load testing](/documentation/guides/testing-agents/load-testing)
