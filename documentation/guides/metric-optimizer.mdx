---
title: "Metric Optimizer"
sidebarTitle: "Metric Optimizer"
icon: "gauge-high"
iconType: "regular"
description: "Metric Optimizer helps you define, test, and refine custom metrics for your AI agents to ensure accurate performance measurement."
---

<iframe
  width="100%"
  height="400px"
  style={{
    aspectRatio: "16 / 9",
    border: "1px solid #ccc",
    borderRadius: "8px",
    marginBottom: "20px",
  }}
  src="https://www.tella.tv/video/cm7vbj9p0001k0ajo6ex71u1o/embed"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
  title="Metric Optimizer Overview"
/>

## Advanced Features of Metric Optimizer

Metric Optimizer lets you define, test, and optimize custom metrics for your AI assistant. This guide covers refining metrics to accurately reflect real-world performance.

### Defining Custom Metrics

Create custom metrics tailored to your use cases. For example, a "Time Zone Confirmation" metric could check if your AI assistant confirms the time zone before scheduling a call.

<img src="/images/metriclab/timezone.png" alt="Time Zone Confirmation" className="rounded-md" />

### Identifying Metric Performance Issues

After deploying metrics, you may notice discrepancies between expected and actual results:

- The AI doesn't confirm the time zone, yet the metric marks it as successful
- The AI confirms the time zone, but the metric marks it as unsuccessful

These inconsistencies indicate your metric definition needs refinement.

### Creating Test Sets for Optimization

Add calls or test runs with unexpected metric results to a test set for optimization.

#### Adding from Call Logs

From the **Calls** page, select individual calls with incorrect metric evaluations and add them to a test set.

<img src="/images/metriclab/addtolab.png" alt="Add to Test Set" className="rounded-md" />

#### Adding from Test Results (Bulk)

Add multiple runs at once from the **Results** page:

1. Navigate to your test results
2. Select the runs you want to add using the checkboxes
3. Click **Add to Lab**
4. Choose which metrics you want to optimize
5. An annotation modal opens where you can set expected values for each run

Use the **Previous** and **Next** buttons to navigate through runs and set the correct expected value (True, N/A, or False) for each metric.

#### Setting Expected Values

In the optimizer view, set the expected outcome for each call or run:

1. Review each call recording
2. Determine what actually happened (e.g., "AI did not confirm the time zone")
3. Set the expected value (e.g., set to "False" if the AI didn't confirm)
4. Your changes are saved automatically

Click the **Variables** button to view all variables and their values used during metric evaluation.

### Optimizing Your Metrics

Once you've built a test set with correct expected values, refine your metric definition:

#### Step 1: Navigate to the optimization section

Go to **Optimize** and select the metric you want to improve.

<img src="/images/metriclab/optimizer.png" alt="Optimize Metric" className="rounded-md" />

#### Step 2: Edit the metric prompt

The interface shows your current accuracy score (e.g., "6 out of 8"), indicating how many test cases your metric correctly evaluates.

#### Step 3: Refine the metric definition

Edit the metric prompt to be more precise. For example, specify that the user must mention a specific time zone for the metric to pass.

<img src="/images/metriclab/promt.png" alt="Accuracy" className="rounded-md" />

#### Step 4: Test and iterate

After editing:

1. Save your changes
2. Run the updated metric against your test set
3. Observe the improvement in accuracy (e.g., from 6/8 to 7/8 or 8/8)
4. Continue refining until you reach your desired accuracy

### Managing Test Sets

Manage test sets directly from the optimizer view:

- **Delete test sets**: Select test sets using the checkboxes, then click **Delete Selected**
- **Remove variables**: Hover over any variable badge and click the X button to remove it

### Benefits of Metric Optimization

With this iterative process you can:

- Improve metric accuracy from 50% to 95% or higher
- Ensure your AI assistant's labels accurately reflect real performance
- Make data-driven decisions based on reliable metrics
