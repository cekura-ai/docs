---
title: 'Creating Metrics'
description: 'Learn how to create metrics for your agents'
---

# Create Metric API

Create a new metric to evaluate agent performance.

## API Endpoint

| Method | Endpoint |
|--------|----------|
| `POST` | `https://new-prod.vocera.ai/test_framework/v1/metrics-external/` |

## Authentication

Include your API key in the request headers:

| Header | Description |
|--------|-------------|
| `X-VOCERA-API-KEY` | Your API key obtained from the dashboard |

## Request Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `name` | string | Yes | Name of the metric |
| `description` | string | Yes | Description of what the metric evaluates |
| `audio_enabled` | boolean | No | Whether audio analysis is enabled. Defaults to true |
| `prompt_enabled` | boolean | No | Whether to use a custom prompt. Defaults to true |
| `prompt` | string | No | Custom evaluation prompt template when prompt_enabled is true |
| `agent` | integer | Yes | ID of the agent to evaluate |
| `eval_type` | string | Yes | Type of metric (binary_workflow_adherence, binary_qualitative, continuous_qualitative, numeric, enum) |
| `enum_values` | array | No | List of possible values when eval_type is "enum" |
| `display_order` | integer | No | Order for displaying the metric. Defaults to next available order |

### Example Request Body

```json
{
    "name": "Appointment Booked",
    "description": "Was the appointment booked successfully?",
    "audio_enabled": true,
    "prompt_enabled": true,
    "prompt": "Go through this transcript and check if appointment was booked successfully:\n\n\n{transcript}\n",
    "agent": 1,
    "eval_type": "binary_qualitative",
    "enum_values": [],
    "display_order": 10
}
```

## Response

A successful request returns the created metric details.

### Response Fields

| Field | Type | Description |
|-------|------|-------------|
| `id` | integer | Unique identifier for the metric |
| `agent` | integer | ID of the associated agent |
| `name` | string | Name of the metric |
| `description` | string | Description of the metric |
| `eval_type` | string | Type of metric (binary_workflow_adherence, binary_qualitative, continuous_qualitative, numeric, enum) |
| `enum_values` | array | List of possible enum values when eval_type is "enum" |
| `audio_enabled` | boolean | Whether audio analysis is enabled |
| `prompt_enabled` | boolean | Whether custom prompt is enabled |
| `prompt` | string | Custom evaluation prompt template when prompt_enabled is true |
| `display_order` | integer | Order for displaying the metric |
| `overall_score` | number | Total testsets passed (null if never run in testset) |
| `total_score` | number | Reviewed in total testsets |

### Example Response

```json
{
    "id": 38,
    "agent": 1,
    "name": "Appointment Booked",
    "description": "Was the appointment booked successfully?",
    "eval_type": "binary_qualitative",
    "enum_values": [],
    "audio_enabled": true,
    "prompt_enabled": true,
    "prompt": "Go through this transcript and check if appointment was booked successfully:\n\n\n{transcript}\n",
    "display_order": 10,
    "overall_score": null,
    "total_score": 0
}
```

## Code Examples

<CodeGroup>

```bash Curl
curl -X POST https://new-prod.vocera.ai/test_framework/v1/metrics-external/ \
  -H "X-VOCERA-API-KEY: <your-api-key-here>" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Example Metric Name",
    "description": "Description of what this metric evaluates", 
    "audio_enabled": true,
    "prompt_enabled": true,
    "prompt": "Custom prompt template for evaluation:\n\n{transcript}\n",
    "agent": 123,
    "eval_type": "binary_qualitative",
    "enum_values": [],
    "display_order": 1
  }'
```

```python Python
import requests

def create_metric(
        api_key, 
        name, 
        description, 
        eval_type,
        agent_id,
        audio_enabled=False,
        prompt_enabled=False,
        prompt=None,
        enum_values=[], 
        display_order=None
    ):
    url = 'https://new-prod.vocera.ai/test_framework/v1/metrics-external/'
    
    headers = {
        'X-VOCERA-API-KEY': api_key,
        'Content-Type': 'application/json'
    }
    
    data = {
        'name': name,
        'description': description,
        'eval_type': eval_type,
        'agent': agent_id
    }

    if audio_enabled:
        data['audio_enabled'] = audio_enabled
    if prompt_enabled:
        data['prompt_enabled'] = prompt_enabled
    if prompt:
        data['prompt'] = prompt
    if enum_values:
        data['enum_values'] = enum_values
    if display_order:
        data['display_order'] = display_order


    response = requests.post(url, headers=headers, json=data)
    return response.json()
```

```javascript JavaScript
async function createMetric(
  apiKey,
  name,
  description,
  evalType,
  agentId,
  audioEnabled = false,
  promptEnabled = false,
  prompt = null,
  enumValues = [],
  displayOrder = null
) {
  const url = 'https://new-prod.vocera.ai/test_framework/v1/metrics-external/';
  
  const data = {
    name,
    description,
    eval_type: evalType,
    agent: agentId
  };

  if (audioEnabled) data.audio_enabled = audioEnabled;
  if (promptEnabled) data.prompt_enabled = promptEnabled;
  if (prompt) data.prompt = prompt;
  if (enumValues.length > 0) data.enum_values = enumValues;
  if (displayOrder) data.display_order = displayOrder;

  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'X-VOCERA-API-KEY': apiKey,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify(data)
  });

  return await response.json();
}
```

</CodeGroup>
