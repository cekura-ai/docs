---
title: 'Creating Bulk Metrics'
description: 'Learn how to create multiple metrics at once for your agents'
---

# Create Bulk Metrics API

Create multiple metrics simultaneously for an agent.

## API Endpoint

| Method | Endpoint |
|--------|----------|
| `POST` | `https://new-prod.cekura.ai/test_framework/v1/metrics-external/bulk_create/` |

## Authentication

Include your API key in the request headers:

| Header | Description |
|--------|-------------|
| `X-VOCERA-API-KEY` | Your API key obtained from the dashboard |

## Request Body

The request body should be an array of metric objects. Each metric object has the following parameters:

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `name` | string | Yes | Name of the metric |
| `description` | string | Yes | Description of what the metric evaluates |
| `agent` | integer | Yes | ID of the agent to evaluate |
| `eval_type` | string | Yes | Type of metric (binary_workflow_adherence, binary_qualitative, continuous_qualitative, numeric, enum) |
| `enum_values` | array | No | List of possible values when eval_type is "enum" |
| `audio_enabled` | boolean | No | Whether audio analysis is enabled. Defaults to false |
| `prompt_enabled` | boolean | No | Whether to use a custom prompt. Defaults to false |
| `prompt` | string | No | Custom evaluation prompt template when prompt_enabled is true |
| `evaluation_trigger` | string | No | Trigger type for evaluation (always, automatic, custom) |
| `evaluation_trigger_prompt` | string | No | Custom trigger prompt when evaluation_trigger is "custom" |

### Example Request Body

```json
[
    {
        "name": "Introduction Clarity",
        "description": "This metric evaluates whether the AI voice agent clearly introduces itself...",
        "agent": 1,
        "eval_type": "binary_workflow_adherence"
    },
    {
        "name": "User Response Handling",
        "description": "This metric assesses whether the AI voice agent waits for user responses...",
        "agent": 1,
        "eval_type": "binary_workflow_adherence"
    }
]
```

## Response

The API returns an array of created metric objects.

### Response Fields

| Field | Type | Description |
|-------|------|-------------|
| `id` | integer | Unique identifier for the metric |
| `agent` | integer | ID of the associated agent |
| `name` | string | Name of the metric |
| `description` | string | Description of the metric |
| `eval_type` | string | Type of metric |
| `enum_values` | array | List of possible enum values |
| `audio_enabled` | boolean | Whether audio analysis is enabled |
| `prompt_enabled` | boolean | Whether custom prompt is enabled |
| `prompt` | string | Custom evaluation prompt template |
| `display_order` | integer | Order for displaying the metric |
| `overall_score` | number | Total testsets passed |
| `total_score` | number | Reviewed in total testsets |

### Example Response

```json
[
    {
        "id": 49,
        "agent": 1,
        "name": "Introduction Clarity",
        "description": "This metric evaluates whether the AI voice agent clearly introduces itself...",
        "eval_type": "binary_workflow_adherence",
        "enum_values": [],
        "audio_enabled": false,
        "prompt_enabled": false,
        "prompt": "",
        "display_order": 20,
        "overall_score": null,
        "total_score": 35
    },
    {
        "id": 50,
        "agent": 1,
        "name": "User Response Handling",
        "description": "This metric assesses whether the AI voice agent waits for user responses...",
        "eval_type": "binary_workflow_adherence",
        "enum_values": [],
        "audio_enabled": false,
        "prompt_enabled": false,
        "prompt": "",
        "display_order": 21,
        "overall_score": null,
        "total_score": 35
    }
]
```

## Code Examples

<CodeGroup>

```bash Curl
curl -X POST https://new-prod.cekura.ai/test_framework/v1/metrics-external/bulk_create/ \
  -H "X-VOCERA-API-KEY: <your-api-key-here>" \
  -H "Content-Type: application/json" \
  -d '[
    {
      "name": "Example Metric",
      "description": "Example description",
      "agent": 123,
      "eval_type": "binary_workflow_adherence"
    }
  ]'
```

```python Python
import requests

def create_bulk_metrics(api_key, metrics):
    url = 'https://new-prod.cekura.ai/test_framework/v1/metrics-external/bulk_create/'
    
    headers = {
        'X-VOCERA-API-KEY': api_key,
        'Content-Type': 'application/json'
    }

    response = requests.post(url, headers=headers, json=metrics)
    return response.json()
```

```javascript JavaScript
async function createBulkMetrics(apiKey, metrics) {
  const url = 'https://new-prod.cekura.ai/test_framework/v1/metrics-external/bulk_create/';
  
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'X-VOCERA-API-KEY': apiKey,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify(metrics)
  });

  return await response.json();
}
```

</CodeGroup>
