---
title: "Load Testing"
sidebarTitle: "Load Testing"
icon: "gauge-high"
iconType: "regular
"
description: "Learn how to perform load testing on your AI agents to ensure they can handle high traffic volumes"
---

## Overview

Load testing involves gradually increasing the number of concurrent calls to your agent to identify performance bottlenecks and ensure your system can handle expected traffic volumes. Our platform uses three key metrics for load testing:

- **Talk Ratio**: Measures the percentage of time the agent is speaking
- **Infrastructure Issues**: Detects any technical problems during calls
- **Latency**: Measures response time performance

## Prerequisites

Before starting load testing, ensure you have:

- Access to the Cekura platform
- An active agent configured for testing
- Appropriate parallel call limits for your organization

## Step-by-Step Load Testing Process

### Step 1: Create a New Agent

1. Navigate to your Cekura dashboard
2. [Create a new agent specifically for load testing](/documentation/key-concepts/agents/overview#creating-your-first-agent)
3. Configure the agent with your desired settings
4. Ensure the agent is properly set up and functional

### Step 2: Generate Test Scenarios

1. Generate 10 test scenarios for your agent
2. **Important**: Make sure to disable the "generate expected outcome" button
3. Review the scenarios to ensure they represent realistic use cases
4. Save the scenarios for your load testing

### Step 3: Configure Metrics

By default, the platform will use only these three metrics for load testing:

- Talk ratio
- Infrastructure issues
- Latency

You don't need to worry about configuring additional metrics for load testing purposes.

### Step 4: Initial Test Run

1. Run the 10 test cases once to establish a baseline
2. Monitor the results for any immediate issues
3. Verify that all scenarios execute successfully
4. Note the performance metrics from this initial run

### Step 5: Gradual Load Increase

1. **Start Small**: If the initial run is successful, increase the number of times to run each test case to 2 or 3
2. **Monitor Performance**: Watch for any degradation in talk ratio, infrastructure issues, or latency
3. **Incremental Scaling**: Slowly increase the number of runs up to 100
4. **Performance Monitoring**: At each step, ensure the system maintains acceptable performance levels

To set the frequency (number of times to run each test case), use the frequency parameter when running your evaluators:

<img src="/images/call-limit.png" alt="Setting Frequency for Load Testing" />

<Note>
  For testing higher loads (more calls in parallel), please contact support.
</Note>
